# 50.007 Machine Learning

Algorithms that improve their performance at some task with experience



Why study Machine Learning?

- Easier to build a learning system that to hardcode stuff

- Improve on existing programs
- Adaptive based on environment (robustness)



learning general models from a data set











### General terms and housekeeping

### `Hyperplane`

In geometry, a hyperplane is **a subspace whose dimension is one less than that of its ambient space**. If a space is 3-dimensional then its hyperplanes are the 2-dimensional planes, while if the space is 2-dimensional, its hyperplanes are the 1-dimensional lines.

<p align="center">
  <img width="auto" height="auto" src="./assets/hyperplane.png">
</p>














## Introduction


### Training and Testing

Training is the process of making system able to learn

<p align="center">
  <img width="auto" height="auto" src="./assets/training.png">
</p>

- training set and data set has to come from same distribution

- need to make some assumption or bias



### Performance

factors affecting performance

- Quality of data
- Background knowledge (info you already know that you can set zhijie set outliers or account for stuff)
- Type of feedback provided
- Learning algorithm



2 important factors:

- Modelling
- Optimisation



### Types of ML

- Supervised learning (need human to teach)

  > classification (need to tell machine this is human, building, dog, cat)
  >
  > regression

- Unsupervised learning (autonomously extract patterns)

  > clustering
  >
  > probability distribution esitmation
  >
  > Finding association (in features)
  >
  > Dimension reduction

- Semi supervised learning

  > There is labelled data within your dataset. 
  > First train model with labelled data
  >
  > Then use trained model to predict labels for the unlabelled data

- Reinforcement learning 
  (term is taken from psychology : when robot/dog does correct thing, reward it)
  
  > Decision making





Under Supervised learning there are multiple dimensions of classification. This just means that they have different variables to measure by.

1d classification

2d classification



### Classification

Automated methods that are capable of predicting whether something is A or B given a sample of A and B.



##### Classifier

Something that distinguishes A from B. A classifier can be either Linear and Non-Linear.



##### Linear Classifier

A linear classifier is just a line separating 2 sets of variables

non linear classifiers exist as well! it looks like a snek or anything that isn't a straight line



unsupervised learning



### Applications











------



## Supervised Learning

Given a dataset and training sample of tumor size, texture, perimeter (input) and outcome of recurrent or non recurrent and time (outputs)

<p align="center">
  <img width="auto" height="auto" src="./assets/sup.png">
</p>

Columns are called input variables or features or attributes

​									h : X -> Y (do math on this)



such that h(x) is a good predictor for the value of y

h is called a classifier







### Steps to solving a Supervised learning problem

1) Decide what the input-output pairs are

2) Decide how to encode inputs and outputs

   > This defines the input space X and the output space Y

3. Choose hypothesis class H (model)

4. Choose an error function (cost function) to define the best hypothesis

5. Choose an algorithm for searching efficiently through the space of hypotheses (optimising)





set of classifiers H : modelling

> Different settings of parameters give different classifiers in the set







robustness : some noisy data may be misclassified. A dog may be classified as a cat by mistake. Too much noise, model cant learn as well. 





##### Binary Classification

e.g. email ∈ {spam, not spam}

you wish to find the function `f` that maps x to y



x → `f` → y ∈ {+1, -1}





##### Regression

(location, year → housing price)

x → `f` → y ∈ ℝ



##### Learning

take a data and produce a predictor so that you can do inference



##### inference

how do you compute y given x



<p align="center">
  <img width="auto" height="auto" src="./assets/ml1.png">
</p>

where D_train is dataset









## Linear Classification



##### Feature Vector `Φ(x)`

Originally, features are like, white or black, contains hair or fur, that kind of bs.

Let x denote the original object.



Then it goes to show :

<p align="center">
  <img width="auto" height="auto" src="./assets/feat_vec.png">
</p>

For an input x, its feature vector is 

​									Φ(x) = [Φ1(x),... ,Φd(x)]



Firstly, stop thinking of features as properties of input describing your object.

Rather, think of features as mathematical objects now.



so in particular, `Φ(x)` is a point in a high-dimensional space

where d represents the number of features that your object has in the formula Φ(x) ∈ ℝ^d^  



So if you have 2 features, that would be a point in 2d space, 3 features, 3d space, and 100 features, 100d space.











<p align="center">
  <img width="auto" height="auto" src="./assets/lin2.png">
</p>

where :

h is the classifier

x is the input or training data or object of interest

θ is the weights

θ • x is the score

sign assigns +1 or -1 depending on whether the score ≥ 0 or not. It is a step function









θ orthogonal to decision boundary. As I move θ, it moves the decision boundary

<p align="center">
  <img width="auto" height="auto" src="./assets/lin1.png">
</p>

Linear Classifier that achieves zero training error is called realisable













Tl;dr

| Name              | Formula                                                  |
| ----------------- | -------------------------------------------------------- |
| Feature Vector    | Φ(x) ∈ ℝ^d^                                              |
| Weight Vector     | w ∈ ℝ^d^                                                 |
| Score             | w • Φ(x) ∈ ℝ^d^                                          |
| Linear Classifier | h(x) = sign (<Score>) = {+1 if score >=0, -1 if score <0 |





Definition 1.1  : .... copy from slides











## **Perceptron**

**Perceptron update rule**





### **`Theorem 2.1`** 

> **The perceptron update rule converges after a finite number of mistakes when the training examples are linearly separable through origin.**



